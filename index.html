<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title></title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="index.html">
<meta property="og:site_name">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Haoran Xu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="" type="application/atom+xml">
  
  
  
    <!--<link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">-->
    
<link rel="stylesheet" href="css/source_code_pro.css">

  

  <!--<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">-->
  
<link rel="stylesheet" href="css/bootstrap/bootstrap.min.css">


  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
<link rel="stylesheet" href="css/styles.css">

  

  
  <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>-->
  
<script src="js/jquery.min.js"></script>


<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class="active"
                 href="">Home</a></li>
        
          <li><a class=""
                 href="archives/">Archives</a></li>
        
          <li><a class=""
                 href="about/">About</a></li>
        
          <li><a class=""
                 href="cnblog/">Chinese Blog</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title"></h1>
  
    <p class="blog-description">「こんなきれいな星も、やっぱりここまで来てから、見れたのだと思うから。だから・・もっと遠くへ・・」</p>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          
  
    <article id="post-2022-04-01" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2022/04/01/2022-04-01/">From X Macro to FOR_EACH to Cartesian Product Enumeration with C Macro</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2022/04/01/2022-04-01/" class="article-date"><time datetime="2022-04-01T00:00:00.000Z" itemprop="datePublished">2022-04-01</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Quite a while ago I was implementing an interpreter. A common task in the interpreter is to select the correct interpreter function based on the type of the input. Let’s say we want to implement an addition. We might end up with something like below:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Add</span><span class="params">(T* input1, T* input2, T* output)</span> </span>&#123;</span><br><span class="line">    *output = *input1 + *input2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>to implement the operation. At runtime, we want to dispatch to the right function base on the type of the operands. A natural way to do this is to have a static array holding the mapping from the operand type to the function pointer, similar to below:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> Type &#123; Int32, Int64, Double, ... &#125;;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">const</span> x_addOpPtr[] = &#123;</span><br><span class="line">    (<span class="keyword">void</span>*)Add&lt;<span class="keyword">int32_t</span>&gt;, </span><br><span class="line">    (<span class="keyword">void</span>*)Add&lt;<span class="keyword">int64_t</span>&gt;, </span><br><span class="line">    (<span class="keyword">void</span>*)Add&lt;<span class="keyword">double_t</span>&gt;, ...</span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<p>so at runtime we can just read <code>x_addOpPtr[operandType]</code> to obtain the function pointer we want to call.</p>
<h4 id="The-X-Macro">The X Macro</h4>
<p>Although the code above can work, it is clearly too error prone. If we accidentally made a mistake in the order of the list, we are screwed. A better way is the <a href="https://en.wikipedia.org/wiki/X_Macro" target="_blank" rel="noopener">X Macro</a> pattern. We define a “X macro” for all the types:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FOR_EACH_TYPE(X) \</span></span><br><span class="line">    X(Int32, <span class="keyword">int32_t</span>)    \</span><br><span class="line">    X(Int64, <span class="keyword">int64_t</span>)    \</span><br><span class="line">    X(Double, <span class="keyword">double_t</span>) ...</span><br></pre></td></tr></table></figure>
<p>Then, by defining what <code>X(EnumType, CppType)</code> expands to, we can create logic based on our needs. For example, the following code would reproduce the <code>x_addOpPtr</code> array we want:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> X(EnumType, CppType) (void*)Add<span class="meta-string">&lt;CppType&gt; , </span></span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">const</span> x_addOpPtr[] = &#123; </span><br><span class="line">    FOR_EACH_TYPE(X) </span><br><span class="line">    <span class="literal">nullptr</span> </span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> X	<span class="comment">// hygiene </span></span></span><br></pre></td></tr></table></figure>
<p>Note that the final <code>nullptr</code> is needed because our expansion <code>(void*)Negate&lt;CppType&gt;,</code> would generate an extra comma in the end.</p>
<h4 id="The-New-Challenge">The New Challenge</h4>
<p>X Macro solved the above problem, but what if we want to handle, say, a type conversion?</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Src, <span class="keyword">typename</span> Dst&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Cast</span><span class="params">(Src* input, Dst* output)</span> </span>&#123;</span><br><span class="line">    *output = <span class="keyword">static_cast</span>&lt;Dst&gt;(*input);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now we have to generate a two-dimensional array, since we have two types <code>Src</code> and <code>Dst</code> to enumerate on. While X Macro can easily iterate through one list and perform action on every item, it cannot iterate through the <strong>Cartesian product of two lists</strong>. A worse solution, is of course, to manually define a list containing all the <code>&lt;Src, Dst&gt;</code> pairs, so we can do X macro again. But what if the array becomes three dimensional?</p>
<p>After some fruitless Googling and home-making attempts to build a “two dimensional X Macro”, I eventually gave up and switched to <a href="https://github.com/sillycross/PochiVM/blob/master/pochivm/ast_type_helper.h#L635" target="_blank" rel="noopener">an ugly solution</a>. Instead of generating a clean static array, we generate a tree of templated dispatching functions. The function at the <code>i-th</code> level use a dispatch array (built by X macro) to dispatch to the next level’s selector function based on the <code>i-th</code> parameter type. We get the function pointer when we reach the leaf. While this approach works, no doubt it is very ugly, and probably also less performant (I didn’t check if the C++ compiler were able to optimize away all the terrible things).</p>
<h4 id="The-FOR-EACH-Macro">The FOR_EACH Macro</h4>
<p>I used to believe my ugly solution is the best solution one can get without resorting to manually enumerating the Cartesian product. However, today I learnt <a href="https://www.scs.stanford.edu/~dm/blog/va-opt.html" target="_blank" rel="noopener">an interesting approach</a> from David Mazieres, which he calls the <code>FOR_EACH</code> macro.</p>
<p>The semantics of the <code>FOR_EACH</code> macro is pretty clear. Taking a macro <code>X</code> (similar to the <code>X</code> in X Macro) and a comma-separated list of elements <code>e1, e2, ... , en</code>, the <code>FOR_EACH</code> macro invokes <code>X</code> on each <code>e</code> in the list. For example, the addition example would look like:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note that the 'X' is gone, and the list is comma-separated</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TYPE_LIST        \</span></span><br><span class="line">    (Int32, <span class="keyword">int32_t</span>) ,   \</span><br><span class="line">    (Int64, <span class="keyword">int64_t</span>) ,   \</span><br><span class="line">    (Double, <span class="keyword">double_t</span>) ...  </span><br><span class="line">    </span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> X(e) (void*)Add<span class="meta-string">&lt;TUPLE_GET_2(e)&gt; ,</span></span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">const</span> x_addOpPtr[] = &#123; </span><br><span class="line">    FOR_EACH(X, TYPE_LIST) </span><br><span class="line">    <span class="literal">nullptr</span> </span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> X</span></span><br></pre></td></tr></table></figure>
<p>The most important difference between <code>FOR_EACH</code> macro and X Macro is that the <code>FOR_EACH</code> list definition doesn’t take <code>X</code>. Unlike the X Macro, where the macro to call on each element is hardcoded to only pass the element itself, the <code>FOR_EACH</code> macro decoupled “the element to be processed” and “the macro processing the element”. This removes the biggest blocker to implement a macro that can enumerate through Cartesian product of multiple lists.</p>
<p>The core of the trick which allows <code>FOR_EACH</code>’s list definition to get rid of the <code>X</code> lies in the <a href="https://en.cppreference.com/w/cpp/preprocessor/replace#Function-like_macros" target="_blank" rel="noopener">C++20 new feature</a> <code>__VA_OPT__</code>. David Mazieres’ original article is already a good explanation on how the <code>FOR_EACH</code> macro works so I won’t parrot it again. With the main blocker removed, after only a few hours of work, I was able to successfully extend <code>FOR_EACH</code> to support enumerating through the Cartesian product of multiple lists. (By the way, even after implementing it, I still have very little idea on how the C preprocessor works, but <code>clang++ -E</code> is enough to trial-and-error into a working solution).</p>
<h4 id="The-FOR-EACH-CARTESIAN-PRODUCT-Macro">The FOR_EACH_CARTESIAN_PRODUCT Macro</h4>
<p>I call my macro <code>FOR_EACH_CARTESIAN_PRODUCT</code>. As the name suggests, it takes a macro <code>X</code> and one or more lists <code>(L1), ..., (Ln)</code>. Then for each <code>(e1, ..., en)</code> in the Cartesian product <code>L1 x ... x Ln</code> , the macro <code>X(e1, ..., en)</code> is invoked. The elements in the Cartesian product are enumerated in lexical order.</p>
<p>For example, for the type-casting example above, the below code would construct our desired two-dimensional dispatch array:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> X(e1, e2) (void*)Cast<span class="meta-string">&lt;TUPLE_GET_2(e1), TUPLE_GET_2(e2)&gt; ,</span></span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">const</span> x_castOpPtr[] = &#123; </span><br><span class="line">    FOR_EACH_CARTESIAN_PRODUCT(X, (TYPE_LIST), (TYPE_LIST)) </span><br><span class="line">    <span class="literal">nullptr</span> </span><br><span class="line">&#125;;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> X</span></span><br></pre></td></tr></table></figure>
<p>Note that the generated array is one-dimensional, but indexing it is pretty simple: <code>x_castOpPtr[opType1 * numTypes + opType2]</code> will give us the desired function pointer for <code>Src=opType1</code> and <code>Dst=opType2</code>.</p>
<p>The code, which contains both the implementation for <code>FOR_EACH_CARTESIAN_PRODUCT</code> and the above examples <a href="https://sillycross.github.ioassets/2022-04-01/for-each-cartesian-product-macro.cpp.txt" target="_blank" rel="noopener">can be found here</a>. The code is in public domain so feel free to use.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2022/04/01/2022-04-01/" data-id="cl1geclos00099bomh1193mxd" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-10-24" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/10/24/2021-10-24/">Some Random Thoughts</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/10/24/2021-10-24/" class="article-date"><time datetime="2021-10-24T00:00:00.000Z" itemprop="datePublished">2021-10-24</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Recently I attended the <a href="https://2021.splashcon.org/track/splash-2021-oopsla#event-overview" target="_blank" rel="noopener">OOPSLA 2021</a> conference. While I haven’t barely learnt anything from the conference itself, it’s indeed a break from my routine life, and I have probably met more people than the sum of the past few months. So as a result, I had some random thoughts and reflections, which I decide to take notes here before they disappear.</p>
<h4 id="Methodology-of-Decision-Making">Methodology of Decision Making</h4>
<p>It is well known that one should not judge a decision based on its outcome, because we cannot have full awareness of the world (so there are information that we <em>cannot</em> know beforehand), and the world itself also contains many random factors (so we cannot predict the future even with complete information). Therefore, an undesirable outcome does not imply the original decision is wrong or improvable.</p>
<p>However, humans are born irrational. Many never understand the above argument at all. But even within the people who can recognize it, I have seen (on both myself and others) many cognitive pitfalls when applying the argument.</p>
<p><em>Pitfall #1.</em> Only apply when things go wrong. Ego makes people believe in their own decisions. So when things work out, it’s easy to overlook the possibility that the success is only coincidental and the original decision is unjustifiable.<br>
As an extreme example, winning a lottery does not justify the decision of spending large amounts of money buying lotteries (which has a negative expected gain).</p>
<p><em>Pitfall #2.</em> Overlooking the information one can know. Similarly, due to people’s ego, when something fails, it’s easy to use the argument as an excuse to deny responsibility, not realizing that the original decision could have been improved with more investigation and reasoning.<br>
The most obvious examples are decisions made through overconfidence and negligence.</p>
<p><em>Pitfall #3.</em> Overlooking the information one cannot know. As stated earlier, one cannot have complete information of the world, so that “there are information that one cannot know” is also an information that must be considered in decision making.<br>
Examples of this pitfall are “perfect plans” that are designed without backups and leaves no buffer on accidents and errors.</p>
<p>So in short, one should not judge a decision based on its outcome, be it desirable or not; one should judge a decision based on the <em>justifiability</em> of how the decision is reached.</p>
<p>(Interestingly, everything stated above can also be observed in <a href="https://en.wikipedia.org/wiki/Japanese_Mahjong" target="_blank" rel="noopener">Japanese Mahjong</a>)</p>
<h4 id="Methodology-of-Becoming-Productive-Researchers">Methodology of Becoming Productive Researchers</h4>
<p>I am aware that all my research ideas have been produced by pure luck. If one had rewinded time, I’m very doubtful if I could come up with the same ideas again. And I feel it clueless to figure out the “next idea” like some of the PhD students who are more “on the right track” could easily do. So I have been curious how the professors can generate an endless stream of ideas for papers. I happened to have discussed this topic with two professors, so for future reference, I take notes here based on my memory.</p>
<p>Q1: (The context here is theoretical computer science.) The difficulty of figuring out a proof is probably exponential to the number of the steps in the proof. So how can you and your group produce so many long (&gt;50 pages) papers every year?</p>
<p>A1 (Richard): We are not going for particular problems. Instead, we have a set of powerful math tools as building blocks, and we just build things up by gluing the blocks together, without a particular goal in mind. If at some time we found that the thing we built solved some problem, we have a paper. It’s like a naval battle: you don’t search for and destroy a particular ship (problem) on the sea. You patrol the sea and destroy any ship spotted along the way.</p>
<p>Q2: But I assume you still have some intuition on what might work and what might not. What is the intuition that guides you and how did you get this intuition?</p>
<p>A2 (Richard): I don’t know. It’s like the first man who figured out they can put guns on ships.</p>
<p>Q3: (The context here is PL/Compilers, and my advisor primarily works on sparse tensor algebra.) I asked my advisor how he could always have the next paper idea to work on.</p>
<p>A3 (Fred): There are many solved problems in the dense algebra domain, but little is known about the sparse algebra counterparts. <a href="http://fredrikbk.com/publications/taco.pdf" target="_blank" rel="noopener">TACO</a> is a framework for solving problems in the sparse algebra domain, so it opens up a sequence of works by porting the solved problems in dense algebra to sparse algebra.</p>
<h4 id="What-Prevents-Constructive-and-Rational-Discussion">What Prevents Constructive and Rational Discussion?</h4>
<p>The motivation of this part is the (still ongoing!) Chinese Internet shitshow centered around a consipracy theory that the Grand Final match of <a href="https://liquipedia.net/dota2/The_International/2021" target="_blank" rel="noopener">Dota2 TI10</a> is fixed.</p>
<p>I have never held any expectation on the rationality of the mass public, but I’m still astonished by that a conspiracy theory without even a self-coherent story can get dominance in the Chinese Dota2 community.</p>
<p>Though it might be my illusion, but I do feel the Internet discussions I see on contemporary matters have been getting increasingly polarized / emotion-driven, and decreasingly constructive / helpful in the past years. Is the mass public becoming more irrational? I don’t know. But after thinking for a bit, I do feel there are a few contributing factors.</p>
<ol>
<li>It takes much more words and time to refute a conspiracy theory or to post some serious discussion, than to propose a conspiracy theory or to post some trashtalk.</li>
<li>The bandwidth of the Internet has greatly increased, but the bandwidth of the useful information being carried has actually decreased. On one hand, blogs are replaced by tweets, texts are replaced by pictures/videos, so the mass public has been trained to only read short messages, not serious discussions. On the other hand, mobile phones, which are not even designed to type efficiently, have surpassed the market occupation rate of PCs long ago, so it is also harder for the mass public to publish anything other than short messages. So the mass public has been trained to only read 140 characters and post 140 characters, not any serious discussions.</li>
<li>By Pareto principle, 80% of the voice in a community comes from 20% of the people. And the people who feel most compelled to speak out are usually the people holding the most extreme opinions. But under the current shape of the Internet, where serious discussions are unfavored, the megaphone is handed over to the most irrational ones, not the most rational ones.</li>
<li>The ranking mechanism, where contents are ranked by user votes and shown to users by rank, served as another amplifier.</li>
<li>But what about the POLs? Will they send out rational messages and lead the public opinions to the rational side? Unfortunately, at least for the Chinese Internet’s status quo, where most of the POLs are commercialized, the answer is negative. The POLs do not care about anything but making more money, which come from public exposure and supporters. So they have no motivation to argue against the trend at all. In fact, many POLs are known to intentionally start flamewars or spread falsehood messages to gain exposure.</li>
<li>Another interesting factor is the bots. It might be surprising, but a <a href="https://www.cmu.edu/ambassadors/july-2020/covid-falsehoods.html" target="_blank" rel="noopener">CMU research</a> showed that the majority of the COVID falsehood messages on Twitter are spreaded by bots. And while I haven’t seen an academic research for the Chinese Internet, it’s undeniable that there are many keyword-based bots for various purposes (lottery, advertisement, promotion, PR manipulation, etc). It’s not surprising that there are similar falsehood message bots as well.</li>
</ol>
<p>But what exactly went wrong? And how this might be fixed? Honestly I don’t know.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/10/24/2021-10-24/" data-id="cl1geclor00089bom1l7pc2ry" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-09-20" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/09/20/2021-09-20/">Understanding JavaScriptCore&#39;s DFG JIT: CPS Rethreading Pass</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/09/20/2021-09-20/" class="article-date"><time datetime="2021-09-20T00:00:00.000Z" itemprop="datePublished">2021-09-20</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>This is another note on JavaScriptCore (JSC)'s DFG JIT. The topic today is on DFG’s IR design and a related optimizer pass called <em>CPS Rethreading</em>. As before, disclaimer first:</p>
<blockquote>
<p><strong>Disclaimer</strong><br>
I do not work on JavaScriptCore, and my experience with Javascript is very limited. Everything described in this post is from my understanding of the JSC source code. They may be inaccurate or outright wrong. Please feel free to email me at haoranxu510 [at] gmail.com for any corrections and discussions.</p>
</blockquote>
<!-- In my opinion, the most interesting design choices of DFG's IR are centered around how it handles variables. I will start with a quick introduction of DFG's IR. -->
<p>In DFG’s IR, each function consists of a list of <a href="https://en.wikipedia.org/wiki/Basic_block" target="_blank" rel="noopener">basic blocks</a>, and each basic block contains a list of IR intructions. An IR instruction is also implicitly the value this instruction produces, and can be used as operand to other IR instructions. Everything till now is similar to a typical <a href="https://en.wikipedia.org/wiki/Static_single_assignment_form" target="_blank" rel="noopener">SSA representation</a>. However, there are two important differences:</p>
<ol>
<li>Each IR instruction can only use the SSA values (implicitly produced by other IR instructions) in the <strong>same</strong> basic block.</li>
<li>Unlike in SSA, DFG does <strong>not</strong> have an IR instruction named “Phi”. Instead, in DFG’s IR, each function has a list of “slots”, which are local variables that one can read from and write to. There are two IR instructions <code>GetLocal</code> and <code>SetLocal</code><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> which allows reading/writing a slot respectively.</li>
</ol>
<p>As one would expect, a <code>GetLocal</code> takes a slot number constant as operand, and produces an SSA value. A <code>SetLocal</code> takes an SSA value (i.e., an IR instruction that produces a value) and a slot number constant as operands, and produces nothing.</p>
<p>The above representation (called <code>LoadStore</code> form in JSC) is very flexible for users to generate the IR, but not good for optimization, because in such representation it is hard to reason about the data stored in local variables. This is where the CPS Rethreading pass comes in, which transforms the graph to the so-called <code>ThreadedCPS</code> form. The CPS rethreading pass optimizes away redundant loads, adds <strong>auxiliary</strong> Phi nodes that describe the data flow of loads and stores, and also performs <a href="https://en.wikipedia.org/wiki/Live_variable_analysis" target="_blank" rel="noopener">liveness analysis</a>.</p>
<p>The most interesting design choice here is that the Phi nodes are auxiliary and optional, and are <em>not</em> part of the IR graph: the correctness of the IR graph is not affected even if all Phi nodes were removed, and by doing so, it only puts the graph back to <code>LoadStore</code> form.</p>
<p>As a result, a transformation pass may choose to either maintain the Phi nodes and the liveness information, or simply mark that the graph has been put back to <code>LoadStore</code> form (and then the CPS Rethreading pass can be called to bring it to <code>ThreadedCPS</code> form again).</p>
<p>The extra information available in <code>ThreadedCPS</code> form is the following:</p>
<ol>
<li>Each basic block gets a list of auxiliary Phi nodes. Unlike Phi nodes in SSA form, the Phi node here is parametrized by a slot number constant, so it denotes the value of that local variable at the start of the basic block. Those Phi nodes are only referenced by other Phi nodes (from other basic blocks) and by the stuffs below. They are never used by a normal IR instruction as operand.</li>
<li>Each basic block gets two auxiliary arrays <code>variablesAtHead</code> and <code>variablesAtTail</code> of length <code>N</code> (where <code>N</code> is the total number of local variable slots of the function). The <code>variablesAtHead</code> array denotes the value of all <a href="https://en.wikipedia.org/wiki/Live_variable_analysis" target="_blank" rel="noopener">live variables</a> at the start of the basic block. The <code>variablesAtTail</code> array denotes the value of all available (defined here as either live at the start of the basic block, or modified inside the basic block) variables at the end of the basic block.</li>
<li>Each <code>GetLocal</code> node gets an auxiliary pointer which denotes the value this <code>GetLocal</code> would yield. The pointer points to either a value-producing instruction in the same basic block (which means the <code>GetLocal</code> would yield that value), or a Phi node in the same basic block (which means the <code>GetLocal</code> would yield the value of the Phi).</li>
<li>Redundant <code>GetLocal</code>s to the same local variable in the same basic block are removed. There are two cases:<br>
(a). A <code>GetLocal</code> after another <code>GetLocal</code> can be replaced by the previous <code>GetLocal</code>.<br>
(b). A <code>GetLocal</code> after a <code>SetLocal</code> can be replaced by the operand of the <code>SetLocal</code>.</li>
</ol>
<p>There are a few points that are worth to note:</p>
<ol>
<li>Point (2) above implies that the total space consumption is <code>O(NM)</code> where <code>N</code> is the total number of local variable slots and <code>M</code> is the number of basic blocks.</li>
<li>Redundant <code>SetLocal</code>s to the same local variable inside the same basic block are not removed. Probably it has something to do with OSR.</li>
<li>The design above makes sure that all the extra information (most notably, the Phi nodes) are auxiliary: they can be safely dropped without affecting correctness.</li>
</ol>
<h4 id="The-Algorithm">The Algorithm</h4>
<p>The algorithm is implemented in <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGCPSRethreadingPhase.cpp" target="_blank" rel="noopener">DFGCPSRethreadingPhase.cpp</a>. For simplicity, as before, we will focus on <code>GetLocal</code> and <code>SetLocal</code> only, and ignore <code>Flush</code>, <code>PhantomLocal</code>, <code>SetArgumentDefinitely</code> and <code>SetArgumentMaybe</code> related logic.</p>
<p>The first step of the algorithm resets the existing state in case the pass has been invoked earlier. It clears the <code>variablesAtHead</code> and <code>variablesAtTail</code> array for each basic block, removes all Phi nodes, and reserves the space for the annotation pointer for each <code>GetLocal</code> node (by simply repurposes the unused <code>child1</code> field).</p>
<p>In the second step, for each basic block, it iterates all the IR instructions from up to down. The <code>variablesAtTail</code> array is used as a scratchpad to keep track of the current value of each variable at the current IR instruction being iterated.</p>
<ol>
<li>If it is a <code>GetLocal</code> instruction, and <code>variablesAtTail[i]</code> is <code>nullptr</code>, then this is the first time the variable <code>i</code> is used in this basic block, and the value of this variable should come from a Phi node. So we create a Phi node and store it into <code>variablesAtHead[i]</code>. The <code>variablesAtTail[i]</code> should also be updated to the current <code>GetLocal</code> node<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>.</li>
<li>If it is a <code>GetLocal</code> instruction, but <code>variablesAtTail[i]</code> is not <code>nullptr</code>, then the local variable <code>i</code> has been used in this basic block. Thus, this <code>GetLocal</code> can always be removed from the graph by replacing it with another node (with detail described earlier). Thanks to the design of the IR, this <code>GetLocal</code> may only be used by later IR instructions in the same basic block, so the replacement can be done easily<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</li>
<li>If it is a <code>StoreLocal</code> instruction, then we can simply update the corresponding slot in <code>variablesAtTail</code>.</li>
</ol>
<p>The third step builds the incoming edges for all Phi nodes. The Phi nodes created in the previous step are put into a list<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>. The algorithm iterates until the list gets empty. Each time an element, let’s say, a Phi node for some variable <code>i</code> in basic block <code>B</code>, is removed from the list. Then for all predecessors <code>P</code> of basic block <code>B</code>:</p>
<ol>
<li>If <code>P.variablesAtTail[i]</code> is <code>nullptr</code>, then there isn’t a Phi node in <code>P</code> for local variable <code>i</code> yet. So, a new one is created, and <code>P.variablesAtHead[i]</code> and <code>P.variablesAtTail[i]</code> is updated. The newly created Phi node is put into the list.</li>
<li>Otherwise, the predecessor of the current Phi node for that basic block should simply be <code>P.variablesAtTail[i]</code><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>.</li>
</ol>
<p>Note that the second step and the third step also guarantee that <code>variablesAtHead</code> correctly contains the information for all live variables at the start of a basic block.</p>
<h4 id="Conclusion">Conclusion</h4>
<p>In my opinion, the design of the DFG IR is really neat. The standard SSA form is best suited for optimization, but it is both difficult and slow to construct and transform. The DFG IR made interesting design choices that intentionally deviates from the standard SSA form to improve compilation speed and usability. This matches DFG JIT’s design goal as a fast optimizer. The CPS Rethreading pass is a neat algorithm that serves multiple purposes simultaneously: to simplify and canonicalize the IR, to construct a SSA-like Phi data flow, and to perform liveness analysis.</p>
<hr>
<h5 id="Footnotes">Footnotes</h5>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>In fact, there are also <code>Flush</code> and <code>PhantomLocal</code> which are related to OSR, and <code>SetArgumentDefinitely</code> and <code>SetArgumentMaybe</code> which are related to special casing of arguments, but we will overlook them in this article for simplicity. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>It seems like the algorithm intentionally updates it to the <code>GetLocal</code> node instead of the Phi node because it can provide more information to users: one can easily trace to the Phi from the <code>GetLocal</code> but not otherwise around. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Specifically, all we need to do is to maintain the replacement map, and at the time we iterate a node, we check if any of its operands are in the replacement map. If yes, we replace it. The replacement map can simply be a <code>std::map</code>, but a hash table lookup is still slow, so in JSC there is a 8-byte scratch field in each <code>Node</code> struct that can be used for this purpose (or other similar purposes). <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>In fact, there are three different kinds of local variables: <code>Argument</code>, <code>Local</code> and <code>Tmp</code>. So correspondingly, there are three different lists. I think it’s only an implementation detail: the three list could have been merged into one, but the current code design (where the <code>Kind</code> is used as a template parameter in several APIs) makes it easiest to just have three lists. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>The actual implementation is a bit more complex. If the predecessor node is a <code>GetLocal</code>, it would further forward it to the value of the <code>GetLocal</code> (as computed in the auxiliary pointer), but if the predecessor node is a <code>StoreLocal</code>, it won’t forward it to its operand. But these are only implementation details to best fit the needs of the clients (the other optimization passes), so we omit them to highlight the core idea. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/09/20/2021-09-20/" data-id="cl1geclor00079bomd9q079f3" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-09-12" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/09/12/2021-09-12/">Static Analysis in JavaScriptCore (Part I)</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/09/12/2021-09-12/" class="article-date"><time datetime="2021-09-12T00:00:00.000Z" itemprop="datePublished">2021-09-12</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Recently I’ve been spending time to understand some internals of JavaScriptCore (JSC), the Javascript JIT compiler powering the Safari browser. For background knowledge, I strongly recommend <a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/" target="_blank" rel="noopener">this great article from WebKit blog</a> for a detailed overview of JSC.</p>
<p>This series of posts attempts to dive deeper into a specific area of JSC: the static analysis passes in JSC’s DFG JIT. Since Javascript is a highly dynamic langauge, it is critical to obtain as much type information as possible to better understand program behaviors so optimizations can take place. In JSC, static analysis is the primary tool for this purpose.</p>
<!--
As explained in the WebKit blog article, there are three most powerful static analysis passes: 
1. Prediction propagation, which fills in speculated types of all values based on the runtime profile of some values. 
2. Abstract interpreter, which, among other things, allows identifying redundant type speculation checks.
3. Clobberize, which as the name suggests, performs alias analysis. It allows elimination of redundant loads and code motion. 
-->
<p>The WebKit blog article gave a good overview of the analysis passes. However, since static analyses are hueristic algorithms, the concrete algorithm design is as important as (if not more important than) the high-level idea to yield a working solution. I’m also curious about the other static analysis passes performed by JSC that are not covered in the article.</p>
<p>So I decided to dive into the implementation to get a better understanding of the full picture. This turns out to be much harder than I expected, primarily due to the lack of comments in the codebase. So I’m taking notes here for future reference.</p>
<blockquote>
<p><strong>Disclaimer</strong><br>
I do not work on JavaScriptCore, my experience with Javascript is very limited, and I do not have prior experience on static analysis. Everything described in this post is from my understanding of the JSC source code. They may be inaccurate or outright wrong. Please feel free to email me at haoranxu510 [at] gmail.com for any corrections and discussions.</p>
</blockquote>
<h3 id="Specialized-DFG-IR-Node-Type">Specialized DFG IR Node Type</h3>
<p>The first optimization is not an optimization pass, but happens when the DFG IR is generated from the source-of-truth bytecode. For certain operations, there exists a specialized version of IR node type in addition to the general version. For example, <code>ValueAdd</code> handles the general addition that makes no assumption on operand types, while <code>ArithAdd</code> handles the case where both operands are statically known to be numbers (e.g., not <code>String</code> or <code>BigInt</code> or <code>Object</code>).</p>
<p>The logic that selects between the two versions can be found <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp#L5870" target="_blank" rel="noopener">here</a>. As one can see from the code, if the IR node types for the two operands both always return <code>Number</code> result (the <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGNodeType.h" target="_blank" rel="noopener">IR node type list</a> maintains what each node type may return), then an <code>ArithAdd</code> is emitted instead of <code>ValueAdd</code>. Since this is before any analysis or optimizations are run, there isn’t anything fancy here: all it checks is the IR node type.</p>
<p>Side notes:</p>
<ul>
<li>I found <a href="https://bugs.webkit.org/attachment.cgi?id=352782&amp;action=diff" target="_blank" rel="noopener">this diff</a>, which introduced the <code>ValueSub</code> opcode, quite helpful to understand what’s going on here.</li>
<li>I wasn’t able to figure out why the criteria for selecting <code>ArithAdd</code> is <code>op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult()</code>. The <code>if</code>-check rules out result types that are more precise than <code>Number</code> (e.g., <code>Int32</code>, <code>Int52</code> or <code>Double</code>), but I can’t see the reason to rule out such cases.</li>
</ul>
<h3 id="Backward-Propagation-Pass">Backward Propagation Pass</h3>
<p><a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGBackwardsPropagationPhase.cpp" target="_blank" rel="noopener">Backward propagation</a> is the first static analysis pass executed by the DFG JIT.</p>
<p>The backward propagation pass computes five flags for each DFG IR node. The flags are stored as part of [the <code>NodeFlags</code> field]((https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGNodeFlags.h) in each node. The definitions for the five flags are listed below, as per comments in the code:</p>
<ul>
<li><code>UsesAsNumber</code>: whether the node result may be used in a context that observes fractional, or bigger-than-int32, results.</li>
<li><code>UsesAsOther</code>: whether the node result may be used in a context that distinguishes between <code>undefined</code> and <code>NaN</code>.</li>
<li><code>NeedsNegZero</code>: whether the node result may be used in a context that distinguishes between <code>+0</code> and <code>-0</code>.</li>
<li><code>UsesAsInt</code>: whether the node result may be used in a context that prefers (but not requires) <code>int</code> values.</li>
<li><code>UsesAsArrayIndex</code>: whether the node result may be used in a context that strongly prefers <code>int</code> values.</li>
</ul>
<p>For the first three flags, not setting a flag that ought to be set is a correctness issue.</p>
<p>For example, if the flag <code>UsesAsOther</code> is not set, but a node <code>x</code> is actually used in a context that distinguished <code>undefined</code> and <code>NaN</code> (e.g., <code>x + &quot;123&quot;</code>), then the program may be misoptimized, and the user may observe unexpected result for that computation (e.g., getting <code>&quot;NaN123&quot;</code> instead of <code>&quot;undefined123&quot;</code>).</p>
<p>The <code>NeedsNegZero</code> flag seems (<a href="https://bugs.webkit.org/show_bug.cgi?id=113862" target="_blank" rel="noopener">based on this bug report</a>) to enable some <code>-0</code> related optimizations. For example, <code>a + (-0)</code> can be optimized to <code>a</code> by speculating <code>a</code> to be not <code>-0</code>. The speculation is usually worth because <code>-0</code> is rare, but it’s better to not speculate at all: if <code>a + (-0)</code> is part of a larger expression, and we can prove that even if <code>a + (-0)</code> were <code>-0</code>, the result of the full expression would be the same as if it were <code>0</code>, then we can omit the speculation. One example where speculation <code>a != (-0)</code> can be omitted is <code>(a + (-0)) + 1</code>, and one counterexample where speculation <code>a != (-0)</code> is required is <code>1 / (a + (-0))</code>. This is what the flag tries to determine.</p>
<p>The <code>UsedAsNumber</code> flag, based on my guess (<a href="https://trac.webkit.org/changeset/145489/webkit" target="_blank" rel="noopener">from this commit</a>), is designed to enable a special Javascript-specific optimization: Javascript bit-operators cast input to <code>int32</code> and also outputs <code>int32</code>, so certain overflow checks can be eliminated if the output is fed into a bit-operator (for example, in <code>(a + 1) | 0</code>, if <code>a</code> were speculated as an <code>int32_t</code>, the overflow check for <code>a + 1</code> may be omitted). It’s pretty tricky as seen from the related code and I can’t fully understand how the logic and optimization works. Given also that it seems to be a Javascript-specific optimization, I will overlook this flag below.</p>
<p>For the last two flags, however, not setting a flag that ought to be set is merely a performance issue and does not affect correctness.</p>
<p><a href="https://bugs.webkit.org/attachment.cgi?id=229754&amp;action=diff" target="_blank" rel="noopener">The motivation</a> for <code>UseAsArrayIndex</code> flag is that the conversion from a <code>double</code> to <code>int</code> is an expensive instruction, and since it is used as array index, it could be inside a loop and cause a big performance impact, so we should avoid this bad case if possible.</p>
<h4 id="The-Algorithm">The Algorithm</h4>
<p>In the <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGNodeType.h#L579" target="_blank" rel="noopener">initial state</a> of a node, all the five flags are not set. The backward propagation pass is executed to set up the five flags (and it must set the first three flags correctly <em>for correctness</em>).</p>
<p>The backward propagation pass is a monotonic process. Each flag may only be switched from <code>unset</code> to <code>set</code>, but not the other way. This makes sure that a fixed point (where further execution of the pass can result in no state changes) is guaranteed to be eventually reached. However, this also means that if at one point we set a flag which condition actually never happens in reality, there is no way to undo the bad decision.</p>
<p>The pass iterates until the fixpoint is reached. In each iteration, it does the following:</p>
<ul>
<li>Iterate each basic block in reverse order.</li>
<li>Iterate each IR instruction node in the basic block in reverse order.</li>
<li>For the current IR node, use its flags to update the flags of its operand (i.e., children) nodes. The concrete update logic depends on the node type, and is handwritten.</li>
</ul>
<p>Since there are close to 400 different IR node types, for correctness, the <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGBackwardsPropagationPhase.cpp#L156" target="_blank" rel="noopener">default operation</a> (for a node type not handled in the switch-case) is conservative: it sets the first three flags in all its operands.</p>
<p>As another conservative measure, even for node types that are explicitly handled, the default flags to propagate is the flags of the current node. So flags are propagated all the way down by default, and only stopped when the logic is explicitly written to <em>not</em> propagate a flag.</p>
<p>We will use <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGBackwardsPropagationPhase.cpp#L281" target="_blank" rel="noopener">the logic for <code>ValueAdd</code></a> and <a href="https://github.com/WebKit/WebKit/blob/8d5e5fd60f0712a47548a3b84c397836c481db75/Source/JavaScriptCore/dfg/DFGBackwardsPropagationPhase.cpp#L296" target="_blank" rel="noopener">the logic for <code>ArithAdd</code></a> as examples to illustrate how the algorithm works.</p>
<p>For <code>ArithAdd</code>, since we already know the two operands must be <code>Number</code>s, it’s safe to not set the <code>UsesAsOther</code> flag for its operands. And if at least one of the operands is known to be not a <code>-0</code>, then the result of the add must not be a <code>-0</code> as well, so the <code>-0</code> in the operand will not be observable after the add, so in this case it’s safe to not set the <code>NeedsNegZero</code> flag for both operands.</p>
<p><code>ValueAdd</code> is similar to <code>ArithAdd</code>, except that it may take non-<code>Number</code> operands. In that case, <code>undefined</code> and <code>NaN</code> in operand may be distinguished (for example, in <code>x + &quot;123&quot;</code>). Therefore, for <code>ValueAdd</code>, the <code>UsesAsOther</code> flag is only not propagated if at least one of the operand is known to produce numeric results (in that case, if the other side is <code>undefined</code> or <code>NaN</code>, the final result is always the same, so the two are indistinguishable). Note that such criteria is not complete (sufficient but not necessary): but this is as far as static analysis can go.</p>
<p>Side notes:</p>
<ul>
<li>I couldn’t fully understand the purpose or implementations of the <code>UsedAsNumber</code> flag. Any explanations would be welcomed.</li>
<li>The pass did not handle operators like <code>ValueSub</code>, <code>ValueMul</code>, etc. It seems to me the developers forgot to update the pass when those operators are introduced.</li>
</ul>
<p>One interesting thing is about the “conservative” measures implemented in the code to reduce the risk of correctness bugs. If my interpretation that the devs forgot to put <code>ValueSub</code>, <code>ValueMul</code>, etc. into the pass is correct, then on one hand, the conservative measures indeed prevented a correctness bug. But on the other hand, since no correctness bug is produced, it would take a long time to figure out that the pass has become outdated and has been silently producing suboptimal results.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/09/12/2021-09-12/" data-id="cl1geclop00069bomfowm0gxd" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-09-03" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/09/03/2021-09-03/">Some Experiment Notes on Pitching and Storytelling</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/09/03/2021-09-03/" class="article-date"><time datetime="2021-09-03T00:00:00.000Z" itemprop="datePublished">2021-09-03</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>A while ago I had been co-lecturing a small group of OI students with Richard. I was impressed how he could just take over any talk on anything, and he said:</p>
<blockquote>
<p>A professor is someone who can talk on any topic for 90 minutes.</p>
</blockquote>
<p>Well, although I have no idea if I want to (or can) become a professor, talking skill is definitely something useful to learn, as I increasingly recognize its importance.</p>
<p>Recently, I had the opportunity to pitch my works to quite a few people (well, that’s actually a lot of people conditioning on that I am so introvert and most of my interactions are limited within a few people). So I decide to write down some experiment notes for future reference.</p>
<p>After some reflective thinking, I categorized my experience as following:</p>
<ol>
<li>Talking with technical engineers in the same field.</li>
<li>Talking with technical engineers.</li>
<li>Talking with people who do not do engineering themselves.</li>
</ol>
<p>Interestingly, my experience is that talking to people in the second category is the hardest.</p>
<hr>
<h3 id="Talking-with-people-who-do-not-do-engineering">Talking with people who do not do engineering</h3>
<p>A while ago I talked to two startup founders who used to do engineering, but have since transitioned to management roles. It seems like they are most interested in the product side of the story, i.e., the impact of my work on the end product and the end users. They are also interested in the high level technical ideas, but they are happy to overlook any technical detail that they cannot immediately understand.</p>
<p>So it seems to me the important things for such talks is to highlight the motivation and the end results, i.e., convince the audience that my stuffs are practically useful. The high level “how it works” explanation more serves as a method to clear their doubts: they want just enough technical ideas for them to perceive the work and the results as “reasonable”, but not any more detailed than that.</p>
<p>There is one interesting failure to note. During one of the talks, I mentioned my dynamic language JIT constructor idea, and get asked: “The most dominant dynamic language is Javascript and is already well-optimized. Every other language is decading in the market. Why do you want to optimize for dying languages?” I apparently didn’t anticipate this question, and I have never paid attention to the market occupation rate of the langauges. So I had to go hand-waving and say that there are still many existing use cases, and it’s unlikely that they will die out any soon. If I had researched a bit about the market occupation rate, I would have known that his argument on the language market is simply incorrect and produced a better answer.</p>
<p>So another lesson is that since their focuses and views are different from technical engineers, they may ask unexpected questions. But I don’t see a good solution to handle such situations in general.</p>
<hr>
<h3 id="Talking-with-technical-engineers-in-the-same-field">Talking with technical engineers in the same field</h3>
<p>It seems to me these talks are the only cases that I should <em>really</em> elaborate what I did. So probably there’s not much strategy here: all I should do is to introduce the motivation and the results, then elaborate my idea as clearly as possible, and take feedback.</p>
<p>One thing that surprised me is that during a presentation to Google Dart team, a slide containing the assembly my compiler generated attracted a lot of attention. They are actually looking into the assembly to get a better idea on what I did. Not sure how general it is, but it seems like I might have underestimated how much technical detail the audience is willing to dive into.</p>
<hr>
<h3 id="Talking-with-technical-engineers-outside-the-field">Talking with technical engineers outside the field</h3>
<p>This is where most of the failures come from. If I were to summarize, I feel the main reason is that they want to understand how my stuff precisely works from end to end, but the background differences make explanation difficult and confusion easy.</p>
<p>The biggest failure was in one job interview in which I tried to explain to the interviewer some data structure research I did. I tried to introduce my algorithm by first explaining how x-fast trie (which my algorithm is based on) works. The outcome is clear: the interviewer was completely confused and asked more and more questions, and I didn’t even get into the novel things I did before time run out.</p>
<p>A few smaller failures also happened during the other interviews for that job. The situations were pretty similar. The interviewer gets interested at some particular detail of how my stuffs work, I explained but only to make them confused, and I wasn’t able to make them understand before time runs out. I was also a bit skeptical on how much they really understood what I tried to convey. It’s likely that they are confused much earlier than they speak out that they couldn’t understand something.</p>
<p>But unlike the people doing management, engineers won’t be satisfied by only knowing the motivation and the results (this is why such failures only happen when talking to engineers). So it seems to me the lessons are the following:</p>
<ol>
<li>Be very detailed in terms of the problem and the assumptions, i.e., make it very clear on what I have in hand, what I am trying to solve and what I am assuming. A seemingly-obvious assumption could be not obvious at all for someone with a different background.</li>
<li><strong>But</strong>, stay away from all technical details. Explain everything in high level. It’s probably even worth to explain <em>everything</em> by analogy. If they ask about technical details, probably the best answer is to just say it’s going to take too long to explain, and try to answer the question using an analogue.</li>
<li>Get a good estimation on how much time is needed to explain a certain thing in a certain level of detail. For example, it might take 60 minutes to make a normal engineer understand x-fast trie, so doing this in an interview is doomed to be disastrous.</li>
</ol>
<hr>
<h3 id="A-few-other-failures">A few other failures</h3>
<p>Once I tried to convince Richard that a Halide-like approach might be helpful to implement some of his crazy algorithms. It didn’t end up well because I didn’t know Halide that well and I had to think while talking, which turns out to be amazingly hard. In general, I feel it very hard to think about any logical stuffs while talking, so probably it’s best to avoid that.</p>
<p>Another stupid failure happened in a poster session. I underestimated the weather and trembled in the wind as a result. The lesson is clear.</p>
<hr>
<h3 id="Summary">Summary</h3>
<p>It seems that the common parts above is about the right level of technical details. It is beneficial to present in detail when the audience are people in the same field. However, it seems like in other cases, getting into technical detail is a major source of failures, and should be avoided as much as possible. However, obviously, this is only the conclusion drawn from a few experiences.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/09/03/2021-09-03/" data-id="cl1gecloo00059bom7vdidpkh" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-08-23" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/08/23/2021-08-23/">A Trick for Reflection in C++</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/08/23/2021-08-23/" class="article-date"><time datetime="2021-08-23T00:00:00.000Z" itemprop="datePublished">2021-08-23</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Yesterday I got into the following problem. I want to allow certain C++ struct definitions in my code to be reflectively inspected. For example, if I defined a struct <code>S</code> with two <code>int</code> fields <code>a</code> and <code>b</code>, the other parts of my program should be able to know that the struct <code>S</code> contains such two fields with such definitions and can act upon this information.</p>
<p>Trivially, one can solve this problem by maintaining two pieces of code: the original definition and a map like <code>{ 'a': 'int', 'b': 'int'}</code>. But then the two pieces of code must be manually kept in sync, which is the point I want to avoid.</p>
<p>Such use case is known as reflection. Unfortunately the C++ standard does not have native support for reflection. There are <a href="https://en.cppreference.com/w/cpp/experimental/reflect" target="_blank" rel="noopener">paper proposals</a> to support it, but none of the major compilers seem to have implemented them yet.</p>
<p>The problem can also be solved via <a href="https://alexpolt.github.io/type-loophole.html" target="_blank" rel="noopener">a huge hack</a> called “the C++ Type Loophole”. However, it’s unclear why the hack could work, and it’s so hacky that even the C++ standard committee has reached <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_active.html#2118" target="_blank" rel="noopener">a decision that it should be prohibited</a>. So I’m not brave enough to use this hack.</p>
<p>I eventually reached a less hacky (but of course, less powerful) solution. Since there is no native support for reflection, something has to be instrumented into the definitions of the structs. So in my solution, to make a struct reflective, one must use special macro <code>FIELD(type, name)</code> to define the fields: this allows us to automatically add some instrumentation into it. An example is shown below.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    BEGIN_FIELDS_LIST();</span><br><span class="line">    FIELD(<span class="keyword">int</span> a);</span><br><span class="line">    FIELD(<span class="keyword">double</span> b);</span><br><span class="line">    END_FIELDS_LIST();</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>My trick is based on the C <code>__COUNTER__</code> macro. <code>__COUNTER__</code> is a special macro that each time it is encountered, it is replaced by the current value of an internal counter maintained by the compiler, and then the internal counter is incremented. So each <code>__COUNTER__</code> macro is replaced by an unique integer that monotically increases through each occurrance in the program text.</p>
<p>Note that the <code>__COUNTER__</code> macro is replaced on the spot. For example, <code>a ## __COUNTER__ = a ## __COUNTER__ + 1</code> will not increment the variable, since it’s going to be expanded to something like <code>a1 = a2 + 1</code>. So the common use pattern of <code>__COUNTER__</code> is to pass it to another macro as a parameter, as shown below:</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MY_MACRO_IMPL(a, b, counter) ... my impl ...</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MY_MACRO(a, b) MY_MACRO_IMPL(a, b, __COUNTER__)</span></span><br></pre></td></tr></table></figure>
<p>This way, the <code>MY_MACRO_IMPL</code> macro can use its <code>counter</code> parameter as an unqiue integer.</p>
<p>The core of the trick is to use this <code>__COUNTER__</code> macro to specialize templates. For a simple example, let’s assume we want to define structs that we can reflectively retrieve the number of fields in the struct. Then <code>BEGIN_FIELDS_LIST</code> can expands to the following code:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">int</span> o&gt; <span class="class"><span class="keyword">struct</span> __<span class="title">internal</span> :</span> __internal&lt;o - <span class="number">1</span>&gt; &#123; &#125;;</span><br><span class="line"><span class="keyword">template</span>&lt;&gt; <span class="class"><span class="keyword">struct</span> __<span class="title">internal</span>&lt;counter&gt; &#123;</span></span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">static</span> <span class="keyword">size_t</span> N = <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>And each <code>FIELD</code> macro will expand to the normal definition, as well as the following code:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;&gt; <span class="class"><span class="keyword">struct</span> __<span class="title">internal</span>&lt;counter&gt; &#123;</span></span><br><span class="line">  <span class="keyword">constexpr</span> <span class="keyword">static</span> <span class="keyword">size_t</span> N = __internal&lt;counter - <span class="number">1</span>&gt;::N + <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>And the <code>END_FIELDS_LIST</code> macro will expand to the following code:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="keyword">static</span> <span class="keyword">size_t</span> __numFields = __internal&lt;counter&gt;::N;</span><br></pre></td></tr></table></figure>
<p>To summarize, the idea is the following.</p>
<ol>
<li>The <code>BEGIN_FIELDS_LIST</code> will define the general case of a template specialized by an integer <code>o</code>. The general definition will simply inherit whatever information is computed by template <code>o-1</code>. In addition to that, it also defines the recursion boundary condition (in our example, since we want to count the number of fields, <code>N = 0</code>).</li>
<li>Each <code>FIELD</code> defintion specializes <code>__internal&lt;__COUNTER__&gt;</code>, and computes its information by merging the results in <code>counter-1</code> and itself (in our example, <code>N = __internal&lt;counter-1&gt;::N + 1</code>).</li>
<li><code>END_FIELDS_LIST</code> can retrieve the aggregated results in <code>__internal&lt;__COUNTER__&gt;</code>.</li>
</ol>
<p>As one can see, the correctness of the above approach relies on only that each counter is replaced by a monotonically increasing integer. The starting integer value, or if any integer is skipped in the sequence, do not affect the correctness. And this matches exactly the semantics of the <code>__COUNTER__</code> macro in C. So are we good?</p>
<p>One tricky problem arises from translation units. C/C++ compiler works on translation units (C/C++ files). So if a header file containing our definition is included by multiple source files, we may get different counter values in different translation units. In other words, the <code>__internal</code> struct is specialized differently in different translation units. This doesn’t affect our correctness. However, the important thing is that this violates C++'s one-definition rule.</p>
<p>Fortunately, we are not doomed. C++ standard specifies that a constexpr symbol is only emitted if it is used by non-constexpr code. Since the <code>__internal</code> structs are only used to compute our final constexpr result <code>__numFields</code>, the compiler is guaranteed to not emit anything about the <code>__internal</code> structs. So no violations of the one-definition rule can be observed. And if we need to add non-constexpr functions to the <code>__internal</code> struct, we can also mark it as <code>always_inline</code> (which tells the compiler that the function must be inlined <em>for correctness</em>) to make sure nothing about the <code>__internal</code> structs are emitted.</p>
<p>So to conclude, as long as we make sure that the <code>__internal</code> structs are not used elsewhere other than computing the final results (which can be achieved by, for example, making all its members private and all its non-constexpr functions <code>always_inline</code>), we should be fine with C++'s one-definition rule requirement.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/08/23/2021-08-23/" data-id="cl1geclon00049bomdjqz14od" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-08-09" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/08/09/2021-08-09/">The Overhead of Non-native Stacks (AKA, How Amazingly Hard it is to Get a Microbenchmark Done Right)</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/08/09/2021-08-09/" class="article-date"><time datetime="2021-08-09T00:00:00.000Z" itemprop="datePublished">2021-08-09</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Today I got into a question loosely related to my research. Many language VMs choose to implement their own call stack, instead of piggybacking on the native C stack. Probably one advantage of such non-native stacks is that it makes implementing coroutines and stack walking easier.</p>
<p>But this is not my question. Everyone knows in C function calls are implemented by the <code>call</code> instruction and the <code>ret</code> instruction. These machine instructions implicitly modifies the <code>SP</code> stack pointer register and then redirect the control flow. So if we were to use a non-native stack (that is not managed by the <code>SP</code> register), the straightforward way is to simulate the <code>call</code> and <code>ret</code> by <code>jmp</code>s. However, a <code>ret</code> is actually an <strong>indirect</strong> jump, since it jumps to non-literal address (the return address).</p>
<p>It is well known that indirect jumps are slow, since the jump target is hard to speculate, and if a CPU fails to speculate, it fails to run fast. So my question is:</p>
<blockquote>
<p>How much slower is an (inpredictable) indirect jump compared with a <code>ret</code>?</p>
</blockquote>
<p>Unsurprisingly, modern CPUs extensively optimize <code>ret</code> given its pervasive use, through <a href="http://blog.stuffedcow.net/2018/04/ras-microbenchmarks/" target="_blank" rel="noopener">a specialized branch predictor called a Return Address Stack (RAS)</a>. That post also measured various hardware-specific limits of the RAS, and demonstrated the extraordinary performance punishment if a user program made the RAS unhappy. However, it did not measure how an impredictable indirect jump compares with a <code>ret</code>.</p>
<p>So I decided to experiment with it myself. The idea came off my head is the recursive Fibonacci function. Since it has two recursive calls, a <code>ret</code> simulated by indirect <code>jmp</code> will have two possible targets. Importantly, the pattern of which target being taken is complex and it is unlikely that a CPU can predict it.</p>
<p>So I wrote a simple recursive function using the C native stack, and a hand-rolled function using a custom stack. For the hand-rolled function, a recurse is implemented by a <code>goto</code>, and a return is implemented by a <code>computed goto</code>, using the GCC <a href="https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html" target="_blank" rel="noopener">computed goto</a> extension feature, with the key logic shown below (<a href="/assets/2021-08-09/custom-stack.cpp.txt">full benchmark code</a>).</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        code
    </div>
    <div class='spoiler-content'>
        <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">StackFrame</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">void</span>* ret;</span><br><span class="line">  <span class="keyword">int</span> n;</span><br><span class="line">  <span class="keyword">uint64_t</span> tmp;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// key logic</span></span><br><span class="line"><span class="comment">// stacktop is a StackFrame*</span></span><br><span class="line">entry:</span><br><span class="line">  n = stacktop-&gt;n;</span><br><span class="line">  <span class="keyword">if</span> (n &lt;= <span class="number">2</span>) &#123;</span><br><span class="line">    ret = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">goto</span> *stacktop-&gt;ret;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  stacktop++;</span><br><span class="line">  stacktop-&gt;ret = &amp;&amp;after_call1;</span><br><span class="line">  stacktop-&gt;n = n - <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">goto</span> entry;</span><br><span class="line">  </span><br><span class="line">after_call1:  </span><br><span class="line">  stacktop--;</span><br><span class="line">  stacktop-&gt;tmp = ret;</span><br><span class="line">  </span><br><span class="line">  n = stacktop-&gt;n;</span><br><span class="line">  stacktop++;</span><br><span class="line">  stacktop-&gt;ret = &amp;&amp;after_call2;</span><br><span class="line">  stacktop-&gt;n = n - <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">goto</span> entry;</span><br><span class="line">  </span><br><span class="line">after_call2:</span><br><span class="line">  stacktop--;</span><br><span class="line">  ret += stacktop-&gt;tmp;</span><br><span class="line">  <span class="keyword">goto</span> *stacktop-&gt;ret;</span><br></pre></td></tr></table></figure>
    </div>
</div>
<p>One tricky part is that GCC is smart enough to unroll the Fibonacci function into a single recursive call performed in a loop. We do not want this to happen, since the point of this microbenchmark is to have two call sites. Fortunately, by compiling with <code>-O1</code>, GCC won’t perform this unwanted optimization. I also confirmed that the assembly code generated by GCC correctly replicates the computation logic I wanted to test, as seen below.</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        code
    </div>
    <div class='spoiler-content'>
        <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">00:    mov    %rdi,0x18(%rax)   # %rdi &#x3D; 0x30</span><br><span class="line">04:    sub    $0x1,%edx</span><br><span class="line">07:    mov    %edx,0x20(%rax)</span><br><span class="line">0a:    lea    0x18(%rax),%rax</span><br><span class="line">10:    mov    0x8(%rax),%edx    # func entry</span><br><span class="line">13:    cmp    $0x2,%edx</span><br><span class="line">16:    jg     00 </span><br><span class="line">18:    mov    (%rax),%rcx</span><br><span class="line">1b:    mov    %rsi,%rdx         # %rsi &#x3D; 0x1</span><br><span class="line">20:    jmpq   *%rcx</span><br><span class="line">30:    mov    %rdx,-0x8(%rax)</span><br><span class="line">34:    mov    %r8,(%rax)        # %r8 &#x3D; 0x50</span><br><span class="line">37:    mov    -0x10(%rax),%ecx</span><br><span class="line">3a:    lea    -0x2(%rcx),%edx</span><br><span class="line">3d:    mov    %edx,0x8(%rax)</span><br><span class="line">40:    jmp    10 </span><br><span class="line">50:    sub    $0x18,%rax</span><br><span class="line">54:    add    0x10(%rax),%rdx</span><br><span class="line">58:    mov    (%rax),%rcx</span><br><span class="line">5b:    jmp    20 </span><br></pre></td></tr></table></figure>
    </div>
</div>
<p>I ran my microbenchmark code to compute <code>fib(40)=102334155</code>, and the result is as follow:</p>
<blockquote>
<p><strong>NOTE! THIS IS NOT VALID RESULT! Keep reading.</strong><br>
Custom stack (computed goto) took 0.301 seconds.<br>
Native stack took 0.266 seconds.</p>
</blockquote>
<p>So it seems like the non-native stack incurs about 10% overhead for Fibonacci: I know this is not a very accurate measurement, but I (naively) felt that it is good enough. So I decided to clean up my code a little bit for future record. Of course, the story is not that simple…</p>
<p>So, I decided to add a <code>printf</code> in the <code>main</code> function to make the output more informational. Of course, the <code>printf</code> is outside the benchmark timing region. Specifically, all I did is adding the two <code>printf</code> lines, exactly as shown below:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&gt;&gt;&gt;&gt;&gt; line added &gt;&gt;&gt; <span class="built_in">printf</span>(<span class="string">"Benchmarking Fib custom stack..\n"</span>); </span><br><span class="line">  <span class="keyword">uint64_t</span> result;</span><br><span class="line">  &#123;</span><br><span class="line">    AutoTimer t;</span><br><span class="line">    result = FibCustomStack(<span class="number">40</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"%llu\n"</span>, <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>&gt;(result));</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;&gt;&gt; line added &gt;&gt;&gt; <span class="built_in">printf</span>(<span class="string">"Benchmarking Fib native stack..\n"</span>); </span><br><span class="line">  &#123;</span><br><span class="line">    AutoTimer t;</span><br><span class="line">    result = fib(<span class="number">40</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"%llu\n"</span>, <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>&gt;(result));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>I compiled and ran the code again, not expecting any changes in numbers. What astonished me is that my computed goto implementation went from <code>0.30s</code> straight up to <code>0.34s</code>!</p>
<p>Something clearly went wrong. <strong>But what <em>could</em> go wrong by adding two lines of <code>printf</code>?</strong> And the <code>printf</code> is not even in the code being timed!</p>
<p>I first tried to increase the input from <code>40</code> to <code>45</code> so the benchmark computation runs longer. It turns out that, without the additional <code>printf</code>, the computed goto implementation took <code>3.4s</code>. But with the <code>printf</code>, the time increased to <code>3.9s</code>. So the slowdown is not even a fixed slowdown, but scales with input! <strong>WHAT?!</strong></p>
<p>Okay. So what if I delete one <code>printf</code>? I tried to delete the first <code>printf</code>, so the benchmark in question is executed first, and then the extra <code>printf</code> is executed. However, the <code>3.4s</code> vs <code>3.9s</code> slowdown is still there. How come this extra <code>printf</code> can slow down something that is executed <strong>before</strong> it?!</p>
<p>I decided to dump out the full assembly and compare line to line.</p>
<p>Surprisingly, and not surprisingly, the two versions are identical – except a few more lines of assembly at the expected spot that calls the extra <code>printf</code>. I am completely baffled.</p>
<p>Fortunately, even though I’m not sure if deities exist, I am skeptical that there is a deity who is responsible for my CPU’s performance. I soon realized that the code layout must have been changed by the extra <code>printf</code>. So I printed the pointer address of my function <code>FibCustomStack</code>. Indeed, the pointer changed from <code>0x****190</code> to <code>0x****1a0</code> (the higher digits are inheriently random due to <a href="https://en.wikipedia.org/wiki/Address_space_layout_randomization" target="_blank" rel="noopener">ASLR</a>).</p>
<p>Though I’m not a hardware feature expert, I still know that the CPU branch predictor works by hashing some bits in the address. The change in function address clearly also changes the address of all branch targets. Is this what caused the slowdown?</p>
<p>Fortunately this hypothesis is easy to validate. The <code>FibCustomStack</code> function can be easily modified to not reference any global variables/functions, and thus made trivially relocatable. Then, we can relocate this function to different addresses, and check for any change in performance.</p>
<p>I wrote <a href="/assets/2021-08-09/test-func-address.cpp.txt">a simple program</a> to test this hypothesis (Note: the program is architecture-specific. My CPU is <code>Intel Core i7-7700HQ CPU @ 2.80GHz</code>. The program may or may not run on another CPU). It <code>mmap</code>s a memory region as executable, and copies the binary code for the <code>FibCustomStack</code> function to different offsets in the region, and tests its performance. The program currently only test different offsets of the lowest 7 bits (128 combinations), but changing this behavior is easy.</p>
<p>To plot the result, the execution time corresponding to the 128 offsets (<a href="/assets/2021-08-09/rawoutput.txt">raw data</a>) are plotted into a 8x16 matrix, with the 16 columns denote the value of the lower 4 bits of the offset, and the 8 rows denote the higher 3 bits. The matrix is then plotted as a heat map: a darker color means a slower execution time. The fastest of all 128 values is <code>3.33s</code> while the slowest is <code>4.54s</code>: almost 50% slower than the fastest.</p>
<p><img src="/images/2021-08-09/heatmap.png" alt=""></p>
<p>Somehow counter-intuitively, the function address that yields the fastest execution is not a multiple of 8 or 16. As a note, the function itself is compiled with <code>-falign-functions=16 -falign-labels=16</code>, so the offset between all branch targets and the function address is a multiple of 16. GCC treats it as an optimization to align functions and labels to power-of-2 byte boundaries, but this has been proven by the above heatmap to be not always beneficial. Therefore, the actual search space of code layouts is even larger: not only the higher bits of the function address (that is not tested by our simple program) may matter as well, but also internally, the function may arrange its branch targets at different (and not necessarily multiple-of-8 or 16) addresses to make the branch predictor’s hash function happy.</p>
<p>Another counter-intuitive observation from the raw data is, certain function addresses (especially the slow ones) make the execution time much less stable than others, as shown in the <code>stddev</code> tab. I can think of no explanation why such macro-scale fluctuations can happen.</p>
<p>I’m sure someone who knows more about hardware details than I do can explain better. If you have more ideas on what’s happening here, I’d really appreciate if you could send me an email (haoranxu510 [at] gmail.com)!</p>
<h3 id="Conclusion">Conclusion</h3>
<p>Although the experiment is still far from complete, the data collected as of now is already sufficient to disqualify the usefulness of the initial microbenchmark. While seemingly completely valid, it turns out that all we are testing is the performance at a random point inside a huge state space that is coincidentally thrown to us by the compiler.</p>
<p>It’s worth mentioning that the problem is not fixed even if the said microbenchmark were implemented in assembly: even in assembly, you cannot easily specify the address of your function. And due to ASLR, the higher address bits of your function is always unknown. And if the CPU branch predictor hashes physical address instead of virtual address (I don’t know if it’s true or not), then you cannot control your function address even with ASLR disabled.</p>
<p>Therefore, this demonstrates a case where even the expressiveness of C or assembly is not low-level enough to get sensible microbenchmark results. Furthermore, it is extremely easy to get misleaded: <strong>how counter-intuitive it is that the function address of your function can cause a up-to-50% performance impact</strong>!</p>
<p>So, I witnessed again the complexity of modern CPUs, and the difficulty of the attempts to “capture” its behavior through microbenchmarks.</p>
<p>P.S. During the experiment, I also hit a GCC miscompilation bug (<a href="https://sillycross.github.io/assets/2021-08-09/gcc-bug.cpp.txt" target="_blank" rel="noopener">repro</a>) caused by its computed-goto extension. The comment in my repro explains what happened. I’m not fully certain if my code is completely UB-free (I’m not language standard expert), but I guess I will still limit my trust in the GCC computed goto extension as a result.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/08/09/2021-08-09/" data-id="cl1geclok00029bomdcck08ij" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-08-07" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="2021/08/07/2021-08-07/">Motivation for Yet Another Blog</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="2021/08/07/2021-08-07/" class="article-date"><time datetime="2021-08-07T00:00:00.000Z" itemprop="datePublished">2021-08-07</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>I used to maintain a blog while I was in high school, like many others who have participated in <a href="https://en.wikipedia.org/wiki/International_Olympiad_in_Informatics" target="_blank" rel="noopener">Olympiad in Informatics</a> did. I had been used to posting solutions to competition problems I encountered, interesting algorithms I learnt, and slices of my life experience. Yet, also like most of them, I blogged less and less frequently as I entered college, and before too long, I stopped blogging completely.</p>
<p>The precise reason that a blogger stops posting probably varies from person to person. Retrospectively thinking, the reason for my case is not that I am no longer learning new stuffs in college thus having nothing to post. Instead, I <i>was</i> learning a lot. However, I was hardly able to get much new <i>insights</i> from the stuffs I learnt. If Olympiad in Informatics is about creatively making use of the knowledge one have learnt on new problems, then I guess, during my undergraduate years, I have learnt a lot more knowledge, but I did not learn how to creatively apply those knowledge to produce something new – and by parroting theorems, theories and long proofs verbatim, one clearly cannot make an interesting blog post. Even worse, knowledge fades away in time. I don’t think I still remember anything taught in the hardcore math and TCS classes I took at MIT.</p>
<p>This is a hard lesson learnt in the hard way. I hope that by writing this down into a blog post, and by keep posting about what I have learnt (which makes sure that I am getting some insights from it), I can often get reminded to not get into the same pitfalls again.</p>
<p>There are other reasons as well. I saw by chance <a href="https://www.cs.cmu.edu/~mblum/research/pdf/grad.html" target="_blank" rel="noopener">this amusing but thought-provoking post</a> by Manuel Blum, and his argument on the importance of writing with this interesting “proof”:</p>
<blockquote>
<p>You are all computer scientists.<br>
You know what FINITE AUTOMATA can do.<br>
You know what TURING MACHINES can do.<br>
For example, Finite Automata can add but not multiply.<br>
Turing Machines can compute any computable function.<br>
Turing machines are incredibly more powerful than Finite Automata.<br>
Yet the only difference between a FA and a TM is that<br>
the TM, unlike the FA, has paper and pencil.<br>
Think about it.<br>
It tells you something about the power of writing.<br>
<strong>Without writing, you are reduced to a finite automaton.</strong><br>
<strong>With writing you have the extraordinary power of a Turing machine.</strong></p>
</blockquote>
<p>as well as another <a href="https://matt.might.net/articles/grad-student-resolutions/" target="_blank" rel="noopener">blog post by Matt Might</a> on the importance of practicing writing for graduate students. Having been struggling with writing out academia-paper-grade paragraphs recently, hopefully I can improve my writing skills a little bit by posting &gt;_&lt;.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    
      

    

    <footer class="article-footer">
    <!--
      <a data-url="2021/08/07/2021-08-07/" data-id="cl1geclog00009bomhda27lzv" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
    -->
      
      

    </footer>
  </div>
  
</article>



  




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  


  


  

  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="archives/2022/">2022</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="archives/2021/">2021</a><span class="sidebar-module-list-count">7</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="2022/04/01/2022-04-01/">From X Macro to FOR_EACH to Cartesian Product Enumeration with C Macro</a>
        </li>
      
        <li>
          <a href="2021/10/24/2021-10-24/">Some Random Thoughts</a>
        </li>
      
        <li>
          <a href="2021/09/20/2021-09-20/">Understanding JavaScriptCore&#39;s DFG JIT: CPS Rethreading Pass</a>
        </li>
      
        <li>
          <a href="2021/09/12/2021-09-12/">Static Analysis in JavaScriptCore (Part I)</a>
        </li>
      
        <li>
          <a href="2021/09/03/2021-09-03/">Some Experiment Notes on Pitching and Storytelling</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2022 Haoran Xu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<!--<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>-->

<script src="js/bootstrap/bootstrap.min.js"></script>



  
<link rel="stylesheet" href="fancybox/jquery.fancybox.css">

  
<script src="fancybox/jquery.fancybox.pack.js"></script>




<script src="js/script.js"></script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    menuSettings: {
      zoom: "None"
    },
    showMathMenu: false,
    jax: ["input/TeX","output/CommonHTML"],
    extensions: ["tex2jax.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [["\\(", "\\)"]],
      displayMath: [["\\[", "\\]"]]
    }
  });
</script>

<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
 

</body>
</html>
